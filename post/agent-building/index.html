<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.129.0">

  <title>Agent Building: Ollama Hosted Models Token per Second &middot; Greg Herlein</title>

    

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://blog.herlein.com/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://blog.herlein.com/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="https://blog.herlein.com/css/blackburn.css">

  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css">

  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Raleway&display=swap" rel="stylesheet" type="text/css">

  
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

 
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/androidstudio.min.css">
  <script async src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js"></script>
  
  <script>hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="https://blog.herlein.com/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="https://blog.herlein.com/">Herlein</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://blog.herlein.com/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://blog.herlein.com/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://blog.herlein.com/about/"><i class='fa fa-user fa-fw'></i>About Me</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://blog.herlein.com/projects/"><i class='fa fa-user fa-fw'></i>Projects</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://blog.herlein.com/resume/"><i class='fa fa-user fa-fw'></i>Resume</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://blog.herlein.com/staffing/"><i class='fa fa-user fa-fw'></i>Staffing</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/gherlein" rel="me" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
    </li>
    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/gherlein" rel="me" target="_blank"><i class="fab fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2018-2025. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Agent Building: Ollama Hosted Models Token per Second</h1>
  <h2></h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>15 Jun 2025, 00:00</time>
  </div>

  

  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="https://blog.herlein.com/tags/agents">agents</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://blog.herlein.com/tags/ai">ai</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://blog.herlein.com/tags/skills">skills</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://blog.herlein.com/tags/golang">golang</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://blog.herlein.com/tags/ollama">ollama</a>
    
  </div>
  
  

</div>

  <p>There&rsquo;s no way to really understand something unless you dive into first principles.  This is especially true for AI coding agents.  What is the editor doing under the hood?  I try to peel the onion a bit.</p>
<h2 id="desire-use-ollama">Desire: Use Ollama</h2>
<p>I <em>could</em> just keep doing what I&rsquo;ve been doing, and use VSCode with GitHub Copilot.  My company pays for a license and I&rsquo;ve been digging into that.  In fact, I used that to approach building this new tool.</p>
<p>I want to use Ollama to host a local model.  I tried this with both Zed and VSCode and the results were TERRIBLE.  I suspect I was doing something wrong. But it was basically not viable.</p>
<p>But&hellip; that may be by design.  The IDEs these days are loss-leaders to pull you into their revenue model: consumption of models tied to thier platform.</p>
<p>Now, I&rsquo;m not actually accusing them of breaking anything, but I can see how they would not really want to support something that does not generate them revenue or tie one to their ecosystem any more closely.</p>
<h2 id="installing-ollama">Installing Ollama</h2>
<p>Follow the <a href="https://ollama.readthedocs.io/en/quickstart/">instructions here</a> to install Ollama.</p>
<center>
<img src="https://blog.herlein.com/images/ollama-logo.png" width="33%"/>
</center>
<h2 id="my-program">My Program</h2>
<p>To understand what I did, you probably want to see my code <a href="https://github.com/gherlein/goclient">here</a>.  More on the code later.</p>
<h2 id="testing--ollama-hosted-models-token-per-second">Testing:  Ollama Hosted Models Token per Second</h2>
<p>I have two computing platforms that I want to test:  my shiny new MacBookPro with an M4 Pro chip and 48GB of unified memory, and, a new <a href="https://www.amazon.com/dp/B0DWSSZ7Q9?th=1">Beelink SER9 Pro AI Mini PC, AMD Ryzen AI 9 HX 370</a> with 64GB of memory&hellip; that is unlikely to be fully unified.  In fact, half of the exercise is to learn how to use Ollama to host a local model on that kind of computer.  Will it work?  How do you set it up?</p>
<p>Here&rsquo;s the model I chose to test with so that we have apples to apples:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama show qwen2.5-coder:7b
</span></span><span style="display:flex;"><span>  Model
</span></span><span style="display:flex;"><span>    architecture        qwen2
</span></span><span style="display:flex;"><span>    parameters          7.6B
</span></span><span style="display:flex;"><span>    context length      <span style="color:#ae81ff">32768</span>
</span></span><span style="display:flex;"><span>    embedding length    <span style="color:#ae81ff">3584</span>
</span></span><span style="display:flex;"><span>    quantization        Q4_K_M
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  Capabilities
</span></span><span style="display:flex;"><span>    completion
</span></span><span style="display:flex;"><span>    tools
</span></span><span style="display:flex;"><span>    insert
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  System
</span></span><span style="display:flex;"><span>    You are Qwen, created by Alibaba Cloud. You are a helpful assistant.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  License
</span></span><span style="display:flex;"><span>    Apache License
</span></span><span style="display:flex;"><span>    Version 2.0, January <span style="color:#ae81ff">2004</span>
</span></span><span style="display:flex;"><span>    ...
</span></span></code></pre></div><p>ChatGPT said to to expect the following from my Mac:</p>
<pre tabindex="0"><code>| Mode                 | Quantization | Expected TPS (tokens/sec) |
| -------------------- | ------------ | ------------------------- |
| **Prompt**           | `q4_K_M`     | 35–50                     |
| **Generate**         | `q4_K_M`     | 55–85                     |
| **Total End-to-End** |              | \~60–75 average           |
</code></pre><p>MacBookPro:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run qwen2.5-coder:7b --verbose
</span></span><span style="display:flex;"><span>&lt;fed it the contents of the prompt.txt file in my code repository&gt;
</span></span><span style="display:flex;"><span>&lt;inference results removed <span style="color:#66d9ef">for</span> brevity?
</span></span><span style="display:flex;"><span>total duration:       14.808806709s
</span></span><span style="display:flex;"><span>load duration:        28.828542ms
</span></span><span style="display:flex;"><span>prompt eval count:    <span style="color:#ae81ff">51</span> token<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>prompt eval duration: 300.190041ms
</span></span><span style="display:flex;"><span>prompt eval rate:     169.89 tokens/s
</span></span><span style="display:flex;"><span>eval count:           <span style="color:#ae81ff">620</span> token<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>eval duration:        14.477839s
</span></span><span style="display:flex;"><span>eval rate:            42.82 tokens/s
</span></span></code></pre></div><p>Almost 43 TPS seems on the low end, but it&rsquo;s a non-trivial prompt.  I even made sure that I had disabled spotlight so that the prompt eval rate was not affected.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo mdutil -a -i off
</span></span></code></pre></div><p>How about the Ryzen 9 7950X?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run qwen2.5-coder:7b --verbose
</span></span><span style="display:flex;"><span>&lt;fed it the contents of the prompt.txt file in my code repository&gt;
</span></span><span style="display:flex;"><span>&lt;inference results removed <span style="color:#66d9ef">for</span> brevity?
</span></span><span style="display:flex;"><span>total duration:       34.769163118s
</span></span><span style="display:flex;"><span>load duration:        12.838666ms
</span></span><span style="display:flex;"><span>prompt eval count:    <span style="color:#ae81ff">71</span> token<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>prompt eval duration: 543.123423ms
</span></span><span style="display:flex;"><span>prompt eval rate:     130.73 tokens/s
</span></span><span style="display:flex;"><span>eval count:           <span style="color:#ae81ff">584</span> token<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>eval duration:        34.204503309s
</span></span><span style="display:flex;"><span>eval rate:            17.07 tokens/s
</span></span></code></pre></div><p>Ugh.  17 TPS.  That&rsquo;s terrible.</p>
<h2 id="damn---turns-out-rocm-does-not-support-this-gpu">Damn - turns out ROCm does not support this GPU</h2>
<p>ROCm officially targets:
Discrete GPUs: e.g. Radeon RX 5000/6000/7000 series (RDNA)
Some integrated RDNA 2 (gfx1036/1037)
Professional GPUs: Instinct MI200/MI300
As of ROCm 6.4, the gfx1150 (RDNA 3 iGPU like 780M/890M) is still not on the supported list:
🔗 <a href="https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html">https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html</a></p>
<p>Well, that stinks.  End of this effort.</p>
<h2 id="the-road-to-hell-or-what-i-did-before-i-found-it-wasnt-supported">The Road to Hell (or, what I did before I found it wasn&rsquo;t supported)</h2>
<p>Let&rsquo;s re-install Ollama, perhaps I did it incorrectly when I set up the system.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh | sh
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Cleaning up old version at /usr/local/lib/ollama
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Installing ollama to /usr/local
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Downloading Linux amd64 bundle
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################## 100.0%</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to render group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding ollama user to video group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Adding current user to ollama group...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Creating ollama systemd service...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Enabling and starting ollama service...
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Downloading Linux ROCm amd64 bundle
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################## 100.0%</span>
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; The Ollama API is now available at 127.0.0.1:11434.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; Install complete. Run <span style="color:#e6db74">&#34;ollama&#34;</span> from the command line.
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; AMD GPU ready.
</span></span></code></pre></div><p>Retested and it&rsquo;s still 17 TPS.  Hmmm.</p>
<p>Next step:  make sure my GPU is setup and enabled in Ollama. After cloning <a href="https://github.com/ggerganov/llama.cpp">ollama</a> I had to find out what GPU I had:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>rocminfo | grep -A4 <span style="color:#e6db74">&#39;Name:&#39;</span>
</span></span><span style="display:flex;"><span>  Name:                    AMD Ryzen AI <span style="color:#ae81ff">9</span> HX <span style="color:#ae81ff">370</span> w/ Radeon 890M
</span></span><span style="display:flex;"><span>  Uuid:                    CPU-XX
</span></span><span style="display:flex;"><span>  Marketing Name:          AMD Ryzen AI <span style="color:#ae81ff">9</span> HX <span style="color:#ae81ff">370</span> w/ Radeon 890M
</span></span><span style="display:flex;"><span>  Vendor Name:             CPU
</span></span><span style="display:flex;"><span>  Feature:                 None specified
</span></span><span style="display:flex;"><span>  Profile:                 FULL_PROFILE
</span></span><span style="display:flex;"><span>  Float Round Mode:        NEAR
</span></span><span style="display:flex;"><span>  Max Queue Number:        0<span style="color:#f92672">(</span>0x0<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>--
</span></span><span style="display:flex;"><span>  Name:                    gfx1150
</span></span><span style="display:flex;"><span>  Uuid:                    GPU-XX
</span></span><span style="display:flex;"><span>  Marketing Name:          AMD Radeon Graphics
</span></span><span style="display:flex;"><span>  Vendor Name:             AMD
</span></span><span style="display:flex;"><span>  Feature:                 KERNEL_DISPATCH
</span></span><span style="display:flex;"><span>  Profile:                 BASE_PROFILE
</span></span><span style="display:flex;"><span>  Float Round Mode:        NEAR
</span></span><span style="display:flex;"><span>  Max Queue Number:        128<span style="color:#f92672">(</span>0x80<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>--
</span></span><span style="display:flex;"><span>      Name:                    amdgcn-amd-amdhsa--gfx1150
</span></span><span style="display:flex;"><span>      Machine Models:          HSA_MACHINE_MODEL_LARGE
</span></span><span style="display:flex;"><span>      Profiles:                HSA_PROFILE_BASE
</span></span><span style="display:flex;"><span>      Default Rounding Mode:   NEAR
</span></span><span style="display:flex;"><span>      Default Rounding Mode:   NEAR
</span></span><span style="display:flex;"><span>--
</span></span><span style="display:flex;"><span>      Name:                    amdgcn-amd-amdhsa--gfx11-generic
</span></span><span style="display:flex;"><span>      Machine Models:          HSA_MACHINE_MODEL_LARGE
</span></span><span style="display:flex;"><span>      Profiles:                HSA_PROFILE_BASE
</span></span><span style="display:flex;"><span>      Default Rounding Mode:   NEAR
</span></span><span style="display:flex;"><span>      Default Rounding Mode:   NEAR
</span></span></code></pre></div><p>So it&rsquo;s the gfx1150.</p>
<p>I tried to build it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/bin/bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Exit on error</span>
</span></span><span style="display:flex;"><span>set -e
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set your target GPU architecture</span>
</span></span><span style="display:flex;"><span>export AMDGPU_TARGET<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gfx1150&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># you need to set this to whatever YOU have!!!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Ensure dependencies are installed</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;Installing build dependencies...&#34;</span>
</span></span><span style="display:flex;"><span>sudo apt update
</span></span><span style="display:flex;"><span>sudo apt install -y git cmake build-essential rocm-hip-libraries rocm-opencl-runtime
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Clone llama.cpp if it doesn&#39;t already exist</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">[</span> ! -d <span style="color:#e6db74">&#34;llama.cpp&#34;</span> <span style="color:#f92672">]</span>; <span style="color:#66d9ef">then</span>
</span></span><span style="display:flex;"><span>    echo <span style="color:#e6db74">&#34;Cloning llama.cpp...&#34;</span>
</span></span><span style="display:flex;"><span>    git clone https://github.com/ggerganov/llama.cpp.git
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">fi</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cd llama.cpp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Clean previous build (optional)</span>
</span></span><span style="display:flex;"><span>rm -rf build
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Configure the build</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;Configuring with ROCm support for </span>$AMDGPU_TARGET<span style="color:#e6db74">...&#34;</span>
</span></span><span style="display:flex;"><span>cmake -B build -DGGML_HIP<span style="color:#f92672">=</span>ON -DAMDGPU_TARGETS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$AMDGPU_TARGET<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build using all available cores</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;Building llama.cpp with HIP acceleration...&#34;</span>
</span></span><span style="display:flex;"><span>cmake --build build -j<span style="color:#66d9ef">$(</span>nproc<span style="color:#66d9ef">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;Build complete! Run with:&#34;</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;./build/bin/main -m path/to/your-model.gguf -p \&#34;Hello, world\&#34; --gpu-layers 40&#34;</span>
</span></span></code></pre></div><p>Hmmm.  No joy.  Looks like I don&rsquo;t actually have the HIP compiler installed properly.</p>
<p>No, it</p>
<p>So let&rsquo;s go reinstall that:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt install python3-setuptools python3-wheel
</span></span><span style="display:flex;"><span>sudo apt update
</span></span><span style="display:flex;"><span>wget https://repo.radeon.com/amdgpu-install/6.4.1/ubuntu/noble/amdgpu-install_6.4.60401-1_all.deb
</span></span><span style="display:flex;"><span>sudo apt install ./amdgpu-install_6.4.60401-1_all.deb
</span></span><span style="display:flex;"><span><span style="color:#75715e"># amdgpu-install -y --usecase=workstation,rocm</span>
</span></span><span style="display:flex;"><span>amdgpu-install --usecase<span style="color:#f92672">=</span>workstation -y --vulkan<span style="color:#f92672">=</span>pro --opencl<span style="color:#f92672">=</span>rocr
</span></span><span style="display:flex;"><span>dkms status
</span></span><span style="display:flex;"><span><span style="color:#75715e"># expecting: amdgpu/6.12.12-2164967.24.04, 6.8.0-60-generic, x86_64: installed</span>
</span></span><span style="display:flex;"><span>rocminfo
</span></span><span style="display:flex;"><span><span style="color:#75715e"># expecting: ROCk module version 6.12.12 is loaded - and a lot of data</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># but this is the important part</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Name:                    gfx1150</span>
</span></span><span style="display:flex;"><span>clinfo
</span></span><span style="display:flex;"><span><span style="color:#75715e"># expecting:  Name:						 gfx1150</span>
</span></span><span style="display:flex;"><span>/opt/rocm/bin/hipcc --version
</span></span><span style="display:flex;"><span><span style="color:#75715e"># expecting: HIP version: 6.4.43483-a187df25c and some data</span>
</span></span></code></pre></div>
  
  <h4><i class="fas fa-share-alt" aria-hidden="true"></i>&nbsp;Share!</h4>
<ul class="share-buttons">
	<li><a href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.herlein.com%2fpost%2fagent-building%2f" target="_blank" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span class="sr-only">Share on Facebook</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://twitter.com/intent/tweet?source=https%3a%2f%2fblog.herlein.com%2fpost%2fagent-building%2f" target="_blank" title="Tweet"><i class="fab fa-twitter" aria-hidden="true"></i><span class="sr-only">Tweet</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://plus.google.com/share?url=https%3a%2f%2fblog.herlein.com%2fpost%2fagent-building%2f" target="_blank" title="Share on Google+"><i class="fab fa-google-plus" aria-hidden="true"></i><span class="sr-only">Share on Google+</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.tumblr.com/share?v=3&u=https%3a%2f%2fblog.herlein.com%2fpost%2fagent-building%2f" target="_blank" title="Post to Tumblr"><i class="fab fa-tumblr" aria-hidden="true"></i><span class="sr-only">Post to Tumblr</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://pinterest.com/pin/create/button/?url=https%3a%2f%2fblog.herlein.com%2fpost%2fagent-building%2f" target="_blank" title="Pin it"><i class="fab fa-pinterest-p" aria-hidden="true"></i><span class="sr-only">Pin it</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.reddit.com/submit?url=https%3a%2f%2fblog.herlein.com%2fpost%2fagent-building%2f" target="_blank" title="Submit to Reddit"><i class="fab fa-reddit-alien" aria-hidden="true"></i><span class="sr-only">Submit to Reddit</span></a>
	</li>
</ul>


<style>
	ul.share-buttons{
	  list-style: none;
	  padding: 0;
	}

	ul.share-buttons li{
	  display: inline;
	}

	ul.share-buttons .sr-only{
	  position: absolute;
	  clip: rect(1px 1px 1px 1px);
	  clip: rect(1px, 1px, 1px, 1px);
	  padding: 0;
	  border: 0;
	  height: 1px;
	  width: 1px;
	  overflow: hidden;
	}
</style>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="https://blog.herlein.com/post/written-by-ai/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="https://blog.herlein.com/post/written-by-ai/">Creating Blog Posts with AI: My Experience with Zed and Claude 3.5</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
  </div>
</div>


  
  
  
  

  

</div>

</div>
</div>
<script src="https://blog.herlein.com/js/ui.js"></script>
<script src="https://blog.herlein.com/js/menus.js"></script>








<script src="https://blog.herlein.com/js/math-code.js"></script>
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  


</body>
</html>

